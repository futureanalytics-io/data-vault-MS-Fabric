{"cells":[{"cell_type":"markdown","source":["# Data Vault 2.0 for Microsoft Fabric\n","\n","This project provides an automated tool for building and managing Data Vault 2.0 architecture within Microsoft Fabric, abstracting a lot of work into writing SparkSQL queries into configurations (YAML or code), utilizing Materialized Lake Views via PySpark. \n","\n","\n","## Core Architecture\n","1. **Staging Layer**: Raw Data ingestion with added metadata where Raw vault would directly pull data from (Load Date, Record Source, Pre-calculated Hash Keys with SHA1).\n","    - Standard Stage: Processes tables needed by Hubs and Satellites\n","    - Special Link Stage: A dedicated staging for link, pre-calculating hashes and joins, ensuring any future satellites connected to it won't need to do the same\n","\n","2. **Raw Vault**: Contains the basic vault entities\n","    - Hubs: Unique business keys.\n","    - Links: Relationships between business keys.\n","    - Satellites: Descriptive state over time.\n","\n","## Features\n","1. **SQL-Centric Automation**: We use PySpark‚Äôs ```sql()``` function as a powerful \"Code Generator.\" By feeding metadata into SparkSQL, the engine automatically builds and manages your Data Vault entities and Materialized Lake Views (MLVs) without manual coding.\n","\n","2. **Scalable Templating**: Scaling your architecture is as simple as updating a configuration. Whether you prefer using a YAML file or a Python dictionary, the \"Template-First\" approach ensures that adding 100 new Hubs or Links follows the exact same standardized logic.\n","\n","3. **Simplicity & Visibility**: Using Materialized Lake Views provides a \"setup-once\" architecture that is both performant and transparent. Because MLVs are native to Fabric, they appear automatically in the Lineage View, allowing you to visually trace data flow from your Stage tables all the way to your final consumer views.\n","\n","4. **Configuration-as-Code**: Your entire Data Vault design is portable. You can export your definitions into a YAML file to backup your architecture or import them into a new Fabric Workspace, making it easy to sync environments (Dev/Test/Prod) in seconds.\n","\n","### Example Usage\n","```\n","dv = DataVaultManager(\"ingestion_time\") # Get the default column for ingestion timestamp for downstream timestamping\n","\n","dv.add_hub(Hub(\n","    name=\"hub_quotes\",\n","    schema_name=\"silver\",\n","    business_key_columns=[\"id\"],\n","    source_table=\"cc_fabric_lakehouse_new.bronze.hubspot_quotes\"\n",")).add_hub(Hub(\n","    name=\"hub_deals\",\n","    schema_name=\"silver\",\n","    business_key_columns=[\"deals\"],\n","    load_datetime_column=\"load_ts\"  # override the default timestamp column\n","    source_table=\"cc_fabric_lakehouse_new.bronze.deals\"\n",")).add_link(Link(\n","    name=\"link_quote_deal\",\n","    schema_name=\"silver\",\n","    staging_schema=\"staging\",\n","    anchor=LinkAnchor(\n","        table=\"cc_fabric_lakehouse_new.bronze.hubspot_quotes\",  # Anchor table, the table after a FROM sql statement\n","        hub=\"hub_quotes\",\n","        bk_columns=[\"id\"]\n","    ),\n","    hub_mapping=[\n","        LinkHubJoin(\n","            hub=\"hub_deals\",\n","            table=\"cc_fabric_lakehouse_new.bronze.hubspot_deals\", # Join table, the table after a JOIN sql statement\n","            bk_columns=[\"id\"],\n","            join_on={\"value\": \"id\"} # FK/PK of the Anchor table to FK/PK of the Joining table\n","        )\n","    ]\n",")).add_satellite(Satellite(\n","    name=\"sat_quotes\",\n","    schema_name=\"silver\",\n","    parent_hub_or_link=\"hub_quotes\",\n","    descriptive_columns=['id', 'ingestion_time'],\n","    include_mode=False, # excludes the columns in the descriptive_columns parameter as the descriptive column of the satellite\n","    source_table=\"cc_fabric_lakehouse_new.bronze.hubspot_quotes_v1\"\n","))\n","\n","\"\"\"\n","execute: run the config or not\n","verbose: print the SQL strings generated\n","force: force execute even with warnings\n","\"\"\"\n","\n","dv.construct_vault(execute=True, verbose=True, force=True)\n","dv.export_config('Files/vault_config.yaml') # export the Data Vault config to yaml\n","\n","# import yaml config file and run\n","dv = DataVaultManager.from_config(\"Files/vault_config.yaml\")\n","dv.construct_vault(execute=True, verbose=True, force=True)\n","```\n","\n","\n","### Internal Notebooks\n","- **dv_core**: Dataclasses and Enums\n","- **dv_ddl_generator**: DDL (SQL String) Generation for Stage and Vault Entities\n","- **dv_validator**: Config Parsing validations\n","- **dv_utils**: Import-Export util, etc...\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"64e04dcd-fbf9-4553-95aa-0d87600e1b0c"},{"cell_type":"code","source":["%run dv_core"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"finished","livy_statement_state":null,"session_id":null,"normalized_state":"finished","queued_time":"2026-01-29T14:29:10.8980085Z","session_start_time":"2026-01-29T14:29:10.8985555Z","execution_start_time":null,"execution_finish_time":"2026-01-29T14:29:11.0581957Z","parent_msg_id":"20e5ef78-83ed-4137-b7d4-32e34ca61f9b"},"text/plain":"StatementMeta(, , -1, Finished, , Finished)"},"metadata":{}},{"output_type":"error","ename":"InvalidHttpRequestToLivy","evalue":"[TooManyRequestsForCapacity] This spark job can't be run because you have hit a spark compute or API rate limit. To run this spark job, cancel an active Spark job through the Monitoring hub, choose a larger capacity SKU, or try again later. HTTP status code: 430 {Learn more} HTTP status code: 430.","traceback":["InvalidHttpRequestToLivy: [TooManyRequestsForCapacity] This spark job can't be run because you have hit a spark compute or API rate limit. To run this spark job, cancel an active Spark job through the Monitoring hub, choose a larger capacity SKU, or try again later. HTTP status code: 430 {Learn more} HTTP status code: 430.","Referenced notebook stacktrace:","  at Notebook dv_core, cell 1"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"74305fd1-eccd-4e0c-b48c-5bb23547d6f5"},{"cell_type":"code","source":["%run dv_validator"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"1fe321f3-eeee-4571-9a40-0a27ef35c019","normalized_state":"finished","queued_time":"2026-01-29T11:06:21.493833Z","session_start_time":null,"execution_start_time":"2026-01-29T11:06:31.3165889Z","execution_finish_time":"2026-01-29T11:06:31.3494502Z","parent_msg_id":"152ea208-3cd8-4bc7-a519-295eff348e20"},"text/plain":"StatementMeta(, 1fe321f3-eeee-4571-9a40-0a27ef35c019, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"52e791b2-a4c3-490a-9f61-ce14a2a3965d"},{"cell_type":"code","source":["%run dv_ddl_generator"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"1fe321f3-eeee-4571-9a40-0a27ef35c019","normalized_state":"finished","queued_time":"2026-01-29T11:06:21.4957352Z","session_start_time":null,"execution_start_time":"2026-01-29T11:06:31.9089031Z","execution_finish_time":"2026-01-29T11:06:31.9372005Z","parent_msg_id":"8f6db9f3-5096-4f06-bcb0-c72b00da50bc"},"text/plain":"StatementMeta(, 1fe321f3-eeee-4571-9a40-0a27ef35c019, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2f3708fd-f692-4d5d-b353-fd9ffbd0a4d3"},{"cell_type":"code","source":["%run dv_utils"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"1fe321f3-eeee-4571-9a40-0a27ef35c019","normalized_state":"finished","queued_time":"2026-01-29T11:06:21.4974576Z","session_start_time":null,"execution_start_time":"2026-01-29T11:06:32.336881Z","execution_finish_time":"2026-01-29T11:06:32.3765444Z","parent_msg_id":"9d78432f-b996-4bdb-a813-23e79aaaaaa1"},"text/plain":"StatementMeta(, 1fe321f3-eeee-4571-9a40-0a27ef35c019, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a5f72f0f-568a-4c64-b9e1-686161b41d0a"},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","from typing import Dict, List, Optional, Union\n","\n","\n","class DataVaultManager(DDLGenerator, DVUtils, Validator):\n","    def __init__(self, default_load_datetime_column: str):\n","        self._stage = []\n","        self._hubs = []\n","        self._links = []\n","        self._sats = []\n","        self._registered_hubs: Dict[str, RegisteredHub] = {}\n","        self._registered_links: Dict[str, RegisteredLink] = {}\n","        self._generated_queries: List[ParametrizedQuery] = []\n","        self._validation_issues: List[ValidationIssue] = []\n","        self._has_errors = False\n","\n","        self._dedupe = True\n","        self._default_load_datetime_column = default_load_datetime_column\n","        self._source_load_datetime_map: Dict[str, str] = {}\n","        self._source_columns_cache: Dict[str, List[str]] = {}\n","\n","        self.validator = SQLValidator()\n","\n","    # PUBLIC API\n","    def add_hub(self, hub: Hub) -> \"DataVaultManager\":\n","        hub.source_table = self._normalize_source_table(hub.source_table)\n","\n","        if self._check_vault_entity_duplicate(hub.name, self._hubs, \"hub\"):\n","            return self\n","\n","        self._validate_identifier(hub.name, allow_dots=False)\n","        self._validate_identifier(hub.schema_name, allow_dots=False)\n","        self._validate_identifier(hub.source_table, allow_dots=True)  # Allow schema.table\n","        self._validate_batch(hub.business_key_columns)  # Validate all BK columns\n","\n","        self._hubs.append(hub)\n","        return self\n","\n","    def add_link(self, link: Link) -> \"DataVaultManager\":\n","        if self._check_vault_entity_duplicate(link.name, self._links, \"link\"):\n","            return self\n","\n","        if link.name in self._registered_links:\n","            self._add_issue(\n","                WarningSeverity.ERROR,\n","                \"data_vault_rule\",\n","                link.name,\n","                f\"Cannot modify existing link '{link.name}'. Links are immutable once created.\",\n","                {\n","                    \"rule\": \"Data Vault: Links cannot have hubs added after creation\",\n","                    \"suggestion\": \"Create a new link with a different name that includes all required hubs\"\n","                }\n","            )\n","            return self\n","\n","        self._validate_identifier(link.name, allow_dots=False)\n","        self._validate_identifier(link.schema_name, allow_dots=False)\n","        self._validate_identifier(link.anchor.table, allow_dots=True)\n","\n","        all_hubs = self._get_all_hubs()\n","        referenced_hubs = []\n","\n","        if link.source_columns:\n","            self._validate_batch(link.source_columns)\n","\n","        if link.anchor.hub:\n","            self._validate_identifier(link.anchor.hub, allow_dots=False)\n","            referenced_hubs.append(link.anchor.hub)\n","            \n","            if link.anchor.bk_columns:\n","                self._validate_batch(link.anchor.bk_columns)\n","            \n","            if link.anchor.hub not in all_hubs:\n","                self._add_issue(\n","                    WarningSeverity.ERROR,\n","                    \"link\",\n","                    link.name,\n","                    f\"Anchor references non-existent hub '{link.anchor.hub}'\",\n","                    {\"available_hubs\": sorted(all_hubs.keys())}\n","                )\n","\n","        for join in link.hub_mapping:\n","            self._validate_identifier(join.hub, allow_dots=False)\n","            self._validate_identifier(join.table, allow_dots=True)\n","            self._validate_batch(join.bk_columns)\n","            referenced_hubs.append(join.hub)\n","\n","            if join.hub not in all_hubs:\n","                self._add_issue(\n","                    WarningSeverity.ERROR,\n","                    \"link\",\n","                    link.name,\n","                    f\"hub_joins references non-existent hub '{join.hub}'\",\n","                    {\"available_hubs\": sorted(all_hubs.keys())}\n","                )\n","\n","        # Validate minimum hub count\n","        if len(referenced_hubs) < 2:\n","            self._add_issue(\n","                WarningSeverity.ERROR,\n","                \"link\",\n","                link.name,\n","                f\"At least 2 hubs required, found {len(referenced_hubs)}: {referenced_hubs}\"\n","            )\n","\n","        self._links.append(link)\n","        return self\n","\n","    def add_satellite(self, sat: Satellite) -> \"DataVaultManager\":\n","        sat.source_table = self._normalize_source_table(sat.source_table)\n","\n","        if self._check_vault_entity_duplicate(sat.name, self._sats, \"satellite\"):\n","            return self\n","\n","        self._validate_identifier(sat.name, allow_dots=False)\n","        self._validate_identifier(sat.schema_name, allow_dots=False)\n","        self._validate_identifier(sat.parent_hub_or_link, allow_dots=False)\n","        self._validate_identifier(sat.source_table, allow_dots=True)\n","        self._validate_batch(sat.descriptive_columns)\n","\n","        self._validate_satellite_parent(sat)\n","        self._sats.append(sat)\n","        return self\n","\n","    def register_hub(\n","        self, \n","        name: str, \n","        schema_name: str = \"silver\", \n","        business_key_columns: List[str] = None\n","    ) -> \"DataVaultManager\":\n","        \"\"\"Register an existing hub for reference by new satellites/links.\"\"\"\n","        \n","        full_name = f\"{schema_name}.{name}\"\n","        \n","        if not self._check_table_existence(schema_name, name):\n","            self._add_issue(\n","                WarningSeverity.ERROR,\n","                \"register\",\n","                name,\n","                f\"Hub '{full_name}' does not exist in catalog\"\n","            )\n","            return self\n","        \n","        if business_key_columns is None:\n","            self._add_issue(\n","                WarningSeverity.ERROR,\n","                \"register\",\n","                name,\n","                f\"Provide business key or primary keys explicitly for '{full_name}'. \"\n","            )\n","            return self\n","        \n","        self._registered_hubs[name] = RegisteredHub(name, schema_name, business_key_columns)\n","        \n","        self._add_issue(\n","            WarningSeverity.INFO,\n","            \"register\",\n","            name,\n","            f\"Registered existing hub '{full_name}' with keys: {business_key_columns}\"\n","        )\n","        \n","        return self\n","\n","    def register_link(self, name: str, schema_name: str = \"silver\") -> \"DataVaultManager\":\n","        \"\"\"Register an existing link for reference by new satellites.\"\"\"\n","        \n","        full_name = f\"{schema_name}.{name}\"\n","        \n","        if not self._check_table_existence(schema_name, name):\n","            self._add_issue(\n","                WarningSeverity.ERROR,\n","                \"register\",\n","                name,\n","                f\"Link '{full_name}' does not exist in catalog\"\n","            )\n","            return self\n","        \n","        self._registered_links[name] = RegisteredLink(name, schema_name)\n","        \n","        self._add_issue(\n","            WarningSeverity.INFO,\n","            \"register\",\n","            name,\n","            f\"Registered existing link '{full_name}'\"\n","        )\n","        \n","        return self\n","\n","    def construct_vault(self, execute: bool=False, force: bool=False, verbose=False):\n","        \n","        staging_map = self._build_staging_map(self._get_unique_tables())\n","\n","        self._process_entities(staging_map)\n","        \n","        self._validate_all_columns()\n","        self._detect_duplicate_satellites()\n","\n","        all_ddls = self._generate_staging_DDL(staging_map) | self._generate_vault_entity_DDL()\n","\n","        if verbose:\n","            for table, ddl in all_ddls.items():\n","                print(f\"\\n{'='*60}\")\n","                print(f\"-- {table}\")\n","                print(f\"{'='*60}\")\n","                print(ddl)\n","\n","        self._print_validation_summary()\n","\n","        err_count = len([v for v in self._validation_issues if v.severity == WarningSeverity.ERROR])\n","        warn_count = len([v for v in self._validation_issues if v.severity == WarningSeverity.WARNING])\n","\n","        if execute:\n","            if force:\n","                if err_count > 0:\n","                    raise UnresolvedVaultErrors(err_count)\n","                else:\n","                    if warn_count > 0:\n","                        print(f\"Running with warnings ({warn_count} warnings). Proceeding with execution.\")\n","                    self._execute_query(all_ddls)\n","            else:\n","                if err_count > 0 or warn_count > 0:\n","                    raise UnresolvedVaultErrors(err_count + warn_count)\n","                self._execute_query(all_ddls)\n","    \n","    # PIPELINE STAGES\n","    def _build_staging_map(self, source_set: set[str]) -> Dict[str, Stage]:\n","        staging_map: Dict[str, Stage] = {}\n","\n","        # TODO: using dict data type dedupes for us nicely, maybe some way to warn that we deduped the specific columns\n","        for source in source_set:\n","            staging_map[source] = Stage(source_table=source)\n","\n","        return staging_map\n","\n","    def _process_entities(self, staging_map: Dict[str, Stage]) -> None:# Process in order\n","        hub_map = {hub.name: hub for hub in self._hubs}\n","        link_map = {link.name: link for link in self._links}\n","        \n","        self._process_hubs(staging_map)\n","        self._process_links(staging_map)\n","        self._process_satellites(staging_map, hub_map, link_map)\n","\n","    def _process_hubs(self, staging_map: Dict[str, Stage]) -> None:\n","        for hub in self._hubs:\n","            stg = staging_map[hub.source_table]\n","            \n","            self._validate_identifier(hub.hash_key_name, allow_dots=False)\n","            stg.hash_keys[hub.hash_key_name] = hub.business_key_columns\n","            \n","            hub._set_stage(VaultEntityMetadata(\n","                schema=stg.schema_name,\n","                table=stg.name,\n","                columns=[hub.hash_key_name, *hub.business_key_columns]\n","            ))\n","\n","    def _process_links(self, staging_map: Dict[str, Stage]) -> None:\n","        all_hubs = self._get_all_hubs()\n","        \n","        for link in self._links:\n","            link_stage_name = f\"stg_{link.name}\"\n","            link_hash_keys = []\n","            skip_link = False\n","\n","            if link.anchor.hub:\n","                hub = all_hubs.get(link.anchor.hub)\n","                if hub:\n","                    self._validate_identifier(hub.hash_key_name, allow_dots=False)\n","                    link_hash_keys.append(hub.hash_key_name)\n","                else:\n","                    self._add_issue(\n","                        WarningSeverity.ERROR,\n","                        \"link\",\n","                        link.name,\n","                        f\"Anchor references non-existent hub '{link.anchor.hub}'\"\n","                    )\n","                    skip_link = True\n","\n","            for mapping in link.hub_mapping:\n","                hub = all_hubs.get(mapping.hub)\n","            \n","                if hub is None:\n","                    skip_link = True\n","                    continue\n","                \n","                self._validate_identifier(hub.hash_key_name, allow_dots=False)\n","                link_hash_keys.append(hub.hash_key_name)\n","\n","            if skip_link:\n","                continue\n","\n","            # Derive staging schema from anchor table\n","            # anchor = link.anchor\n","            # anchor_schema = anchor.table.split(\".\")[-2] if \".\" in anchor.table else \"bronze\"\n","            # link_stage_name = f\"stg_{link.name}\"\n","\n","            link._set_stage(VaultEntityMetadata(\n","                schema=link.staging_schema,\n","                table=f\"stg_{link.name}\",\n","                columns=link_hash_keys\n","            ))\n","\n","    def _process_satellites(\n","        self,\n","        staging_map: Dict[str, Stage],\n","        hub_map: Dict[str, Hub],\n","        link_map: Dict[str, Link]\n","    ) -> None:\n","        all_hubs = {**hub_map, **self._registered_hubs}\n","        all_links = {**link_map, **self._registered_links}\n","\n","        for sat in self._sats:\n","            stg = staging_map[sat.source_table]\n","            hash_diff_name = f\"hd_{sat.name.replace('sat_', '')}\"\n","            \n","            self._validate_identifier(hash_diff_name, allow_dots=False)\n","\n","            parent = all_hubs.get(sat.parent_hub_or_link) or all_links.get(sat.parent_hub_or_link)\n","\n","            if parent is None:\n","                self._add_issue(\n","                    WarningSeverity.ERROR,\n","                    \"satellite\",\n","                    sat.name,\n","                    f\"Parent '{sat.parent_hub_or_link}' not found during processing\"\n","                )\n","                continue\n","\n","            if sat.hash_column:\n","                stg.hash_keys[parent.hash_key_name] = sat.hash_column\n","            \n","            sat._set_stage(VaultEntityMetadata(\n","                schema=stg.schema_name,\n","                table=stg.name,\n","                columns=[hash_diff_name, parent.hash_key_name]\n","            ))\n","\n","            resolved = self._resolve_satellite_columns(sat)\n","            sat._set_resolved_columns(resolved)\n","    \n","    # LOOKUPS\n","    def _get_all_hubs(self) -> Dict[str, Union[Hub, RegisteredHub]]:\n","        all_hubs = {hub.name: hub for hub in self._hubs}\n","        all_hubs.update(self._registered_hubs)\n","        return all_hubs\n","\n","    def _get_all_links(self) -> Dict[str, Union[Link, RegisteredLink]]:\n","        all_links = {link.name: link for link in self._links}\n","        all_links.update(self._registered_links)\n","        return all_links\n","\n","    def _get_all_parents(self) -> Dict[str, Union[Hub, Link, RegisteredHub, RegisteredLink]]:\n","        return {**self._get_all_hubs(), **self._get_all_links()}\n","\n","    def _get_unique_tables(self) -> set[str]:\n","        sources = set()\n","    \n","        for hub in self._hubs:\n","            sources.add(hub.source_table)\n","        \n","        for sat in self._sats:\n","            sources.add(sat.source_table)\n","        \n","        return sources\n","    \n","    # NORMALIZATION\n","    def _normalize_source_table(self, source_table: str, default_schema: str = \"raw\") -> str:\n","        if not source_table or not isinstance(source_table, str):\n","            raise ValueError(f\"Source table must be a non-empty string, got: {source_table}\")\n","        \n","        # Remove extra whitespace\n","        source_table = source_table.strip()\n","        \n","        # Split by dots\n","        parts = source_table.split(\".\")\n","        \n","        if len(parts) == 1:\n","            # Just table name - add default schema\n","            table = parts[0]\n","            normalized = f\"{default_schema}.{table}\"\n","            \n","            self._add_issue(\n","                WarningSeverity.INFO,\n","                \"normalization\",\n","                source_table,\n","                f\"Source table normalized: '{source_table}' ‚Üí '{normalized}'\",\n","                {\"original\": source_table, \"normalized\": normalized}\n","            )\n","            \n","            return normalized\n","            \n","        elif len(parts) == 2:\n","            # schema.table - already correct format\n","            schema, table = parts\n","            \n","            # Validate parts are not empty\n","            if not schema or not table:\n","                raise ValueError(f\"Invalid source table format: '{source_table}'. Schema and table cannot be empty.\")\n","            \n","            return source_table\n","            \n","        elif len(parts) == 3:\n","            # three-point lakehouse naming/cross-lakehouse\n","            lakehouse, schema, table = parts\n","            \n","            # Validate parts are not empty\n","            if not lakehouse or not schema or not table:\n","                raise ValueError(f\"Invalid source table format: '{source_table}'. lakehouse, schema, and table cannot be empty.\")\n","\n","            return source_table\n","            \n","        else:\n","            raise ValueError(\n","                f\"Invalid source table format: '{source_table}'. \"\n","                f\"Expected: 'table', 'schema.table', or 'catalog.schema.table'\"\n","            )\n","\n","    # COLUMN UTILS\n","    def _get_reserved_columns(self, sat: Optional[Satellite] = None, include_parent_hk: bool = False) -> set:\n","        \n","        reserved = {\n","            'load_datetime', \n","            'record_source', \n","            'load_date', \n","            'load_ts',\n","            self._default_load_datetime_column\n","        }\n","        \n","        if sat:\n","            if sat.load_datetime_column:\n","                reserved.add(sat.load_datetime_column)\n","            \n","            if include_parent_hk:\n","                parent = self._get_all_parents().get(sat.parent_hub_or_link)\n","                if parent:\n","                    reserved.add(parent.hash_key_name)\n","        \n","        return reserved\n","\n","    def _filter_columns(\n","        self,\n","        columns: List[str],\n","        reserved: set,\n","        stage: str,\n","        entity_name: str,\n","        reason: str\n","    ) -> List[str]:\n","        \"\"\"Filter columns against reserved set with case-insensitive matching\"\"\"\n","        \n","        reserved_lower = {r.lower() for r in reserved}\n","        \n","        safe_columns = [\n","            col for col in columns\n","            if col.lower() not in reserved_lower\n","        ]\n","        \n","        excluded = set(columns) - set(safe_columns)\n","        if excluded:\n","            self._add_issue(\n","                WarningSeverity.WARNING,\n","                stage,\n","                entity_name,\n","                f\"Excluded {len(excluded)} column(s): {sorted(excluded)}\",\n","                {\n","                    \"excluded_columns\": sorted(excluded),\n","                    \"reason\": reason\n","                }\n","            )\n","        \n","        return safe_columns\n","\n","    def _resolve_satellite_columns(self, sat: Satellite) -> List[str]:  \n","        reserved = self._get_reserved_columns(sat, include_parent_hk=True)\n","        \n","        if sat.include_mode:\n","            # Include mode: use specified columns\n","            return self._filter_columns(\n","                columns=sat.descriptive_columns,\n","                reserved=reserved,\n","                stage=\"satellite\",\n","                entity_name=sat.name,\n","                reason=\"System columns excluded\"\n","            )\n","        \n","        # Exclude mode: use ALL source columns except specified\n","        source_columns = self._get_source_columns(sat.source_table)\n","        \n","        if not source_columns:\n","            self._add_issue(\n","                WarningSeverity.ERROR,\n","                \"satellite\",\n","                sat.name,\n","                \"Cannot use include_mode=False: failed to retrieve source columns\"\n","            )\n","            return []\n","        \n","        all_excluded = reserved | set(sat.descriptive_columns)\n","        \n","        return self._filter_columns(\n","            columns=source_columns,\n","            reserved=all_excluded,\n","            stage=\"satellite\",\n","            entity_name=sat.name,\n","            reason=\"User exclusion (include_mode=False) + system columns\"\n","        )\n","    \n","    def _resolve_load_datetime_column(self, source_table: str, entity_override: Optional[str] = None) -> str:\n","        \"\"\"Resolve load_datetime column: entity override > cached > default\"\"\"\n","        \n","        # Entity-level override takes priority\n","        if entity_override:\n","            self._source_load_datetime_map[source_table] = entity_override\n","            return entity_override\n","        \n","        # Check cached mapping\n","        if source_table in self._source_load_datetime_map:\n","            return self._source_load_datetime_map[source_table]\n","        \n","        # Validate default exists in source\n","        resolved = self._default_load_datetime_column\n","        source_columns = self._get_source_columns(source_table)\n","        \n","        if source_columns and resolved.lower() not in {c.lower() for c in source_columns}:\n","            self._add_issue(\n","                WarningSeverity.ERROR,\n","                \"staging\",\n","                source_table,\n","                f\"load_datetime column '{resolved}' not found in source\",\n","                {\n","                    \"available_columns\": source_columns,\n","                    \"suggestion\": \"Specify load_datetime_column in entity definition\"\n","                }\n","            )\n","        \n","        return resolved\n","\n","    # SPARK INTERACTIONS\n","    def _check_table_existence(self, schema_name: str, table_name: str):\n","        try:\n","            full_name = f\"{schema_name}.{table_name}\"\n","            return spark.catalog.tableExists(full_name)\n","        except Exception as e:\n","            self._add_issue(\n","                WarningSeverity.WARNING,\n","                \"lakehouse\",\n","                table_name,\n","                f\"Could not check table existence: {e}\"\n","            )\n","            return False\n","\n","    def _infer_hub_business_keys(self, schema_name: str, hub_name: str) -> List[str]:\n","        try:\n","            df = spark.table(f\"{schema_name}.{hub_name}\")\n","            # Exclude known system columns\n","            exclude = {'load_datetime', 'record_source', f\"hk_{hub_name.replace('hub_', '')}\"}\n","            return [c for c in df.columns if c.lower() not in {e.lower() for e in exclude}]\n","        except Exception:\n","            return []\n","\n","    def _get_source_columns(self, source_table: str) -> List[str]:\n","        if source_table in self._source_columns_cache:\n","            return self._source_columns_cache[source_table]\n","\n","        try:\n","            df = spark.table(source_table)\n","            self._source_columns_cache[source_table] = df.columns\n","            return df.columns\n","            \n","        except Exception as e:\n","            self._add_issue(\n","                WarningSeverity.ERROR,\n","                \"staging\",\n","                source_table,\n","                f\"Could not retrieve source columns: {e}\"\n","            )\n","            return []\n","\n","    # EXECUTION AND OUTPUT\n","    def _print_validation_summary(self) -> None:\n","        \n","        if not self._validation_issues:\n","            print(\"‚úÖ No validation issues found\\n\")\n","            return\n","        \n","        severity_config = {\n","            WarningSeverity.ERROR: \"‚ùå ERRORS\",\n","            WarningSeverity.WARNING: \"‚ö†Ô∏è  WARNINGS\",\n","            WarningSeverity.INFO: \"‚ÑπÔ∏è  INFO\"\n","        }\n","\n","        for severity, label in severity_config.items():\n","            issues = [i for i in self._validation_issues if i.severity == severity]\n","            \n","            if not issues:\n","                continue\n","                \n","            print(f\"\\n{label} ({len(issues)}):\")\n","            for issue in issues:\n","                print(f\"   [{issue.stage}] {issue.entity}: {issue.message}\")\n","                if issue.details:\n","                    for key, value in issue.details.items():\n","                        print(f\"      ‚îî‚îÄ {key}: {value}\")\n","\n","    def _execute_query(self, ddl_string_list: Dict[str, str]):\n","        print(\"EXECUTING!\")\n","        \n","        created_count = 0\n","        skipped_count = 0\n","\n","        for table, ddl in ddl_string_list.items():\n","\n","            schema_name, table_name = table.split(\".\")\n","            \n","            # Check if table already exists\n","            if self._check_table_existence(schema_name, table_name):\n","                print(f\"‚è≠Ô∏è  Skipping '{table}' ‚Äî already exists\")\n","                skipped_count += 1\n","                continue\n","            \n","            # Table doesn't exist, create it\n","            print(f\"üî® Creating '{table}'...\")\n","            df = spark.sql(ddl)\n","            display(df)\n","            created_count += 1\n","        \n","        print(f\"\\n‚úÖ Execution complete: {created_count} created, {skipped_count} skipped\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"1fe321f3-eeee-4571-9a40-0a27ef35c019","normalized_state":"finished","queued_time":"2026-01-29T11:06:21.7055581Z","session_start_time":null,"execution_start_time":"2026-01-29T11:06:32.3797273Z","execution_finish_time":"2026-01-29T11:06:32.6640426Z","parent_msg_id":"3c7dca98-058d-419d-8c17-8abd44f8cf3f"},"text/plain":"StatementMeta(, 1fe321f3-eeee-4571-9a40-0a27ef35c019, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ed6e9b42-697f-4eb5-a515-16444a953539"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}