{"cells":[{"cell_type":"code","source":["from typing import Dict, List\n","from tqdm.notebook import tqdm\n","\n","class DDLGenerator:\n","    def _generate_hash_sql(self, hash_config: dict) -> str:\n","\n","        name = hash_config[\"name\"]\n","        columns = hash_config[\"columns\"]\n","\n","        safe_columns = [\n","            f\"COALESCE(NULLIF(UPPER(TRIM(CAST({col} AS STRING))), ''), '^^NULL^^')\"  # coalesce to some kind of string that is unlikely to appear, formatting based on AutomateDV\n","            for col in columns\n","        ]\n","\n","        cols_concat = \", '|', \".join(safe_columns)\n","    \n","        return f\"SHA1(CONCAT({cols_concat})) AS {name}\"\n","\n","    def _generate_staging_DDL(self, staging_map: Dict[str, Stage]) -> List[str]:\n","\n","        generated_ddl_str: Dict[str, str] = {}\n","\n","        # Build override map from entities\n","        source_datetime_overrides = {}\n","        for hub in self._hubs:\n","            if hub.load_datetime_column:\n","                source_datetime_overrides[hub.source_table] = hub.load_datetime_column\n","        for sat in self._sats:\n","            if sat.load_datetime_column:\n","                source_datetime_overrides[sat.source_table] = sat.load_datetime_column\n","\n","        for stage in tqdm(staging_map.values(), desc=\"Generating staging DDL string\", unit=\"stage\", total=len(staging_map)):\n","            hk_sql_string = [\n","                self._generate_hash_sql({\"name\": name, \"columns\": cols}) \n","                for name, cols in stage.hash_keys.items()\n","            ]\n","\n","            # safe_source_columns = self._get_staging_passthrough_columns(stage)\n","\n","            # Resolve load_datetime column\n","            load_dt_col = self._resolve_load_datetime_column(\n","                stage.source_table,\n","                source_datetime_overrides.get(stage.source_table)\n","            )\n","\n","            select_columns = (\n","                # safe_source_columns +\n","                [\"*\"] +\n","                hk_sql_string +\n","                [f\"{load_dt_col} AS load_datetime\"] +\n","                [f\"'{stage.source_table}' AS record_source\"]\n","            )\n","\n","            columns_sql = \",\\n\\t\\t\\t\\t\".join(select_columns)\n","\n","            ddl_string = f\"\"\"\n","            CREATE MATERIALIZED LAKE VIEW IF NOT EXISTS `{stage.schema_name}`.`{stage.name}`\n","            TBLPROPERTIES (delta.enableChangeDataFeed = true)\n","            AS\n","            SELECT \n","                {columns_sql}\n","            FROM {stage.source_table}\n","            \"\"\".strip()\n","\n","            full_name = f\"{stage.schema_name}.{stage.name}\"\n","            generated_ddl_str[full_name] = ddl_string\n","\n","        generated_ddl_str.update(self._generate_link_staging_DDL())\n","\n","        return generated_ddl_str\n","\n","    def _generate_vault_entity_DDL(self) -> List[str]:\n","        \n","        vault_ddls: Dict[str, str] = {}\n","\n","         # for stage in tqdm(staging_map.values(), desc=\"Generating staging DDL string\", unit=\"stage\", total=len(staging_map)):\n","        for hub in tqdm(self._hubs, desc=\"Generating hub DDL string\", unit=\"tables\", total=len(self._hubs)):\n","            ddl_string = f\"\"\"\n","            CREATE MATERIALIZED LAKE VIEW IF NOT EXISTS `{hub.schema_name}`.`{hub.name}`\n","            TBLPROPERTIES (delta.enableChangeDataFeed = true)\n","            AS\n","            SELECT\n","                {', '.join(hub._stage.columns)},\n","                MIN(NAMED_STRUCT('ldts', load_datetime, 'rs', '{hub._stage.schema}.{hub._stage.table}')).ldts AS load_datetime,\n","                MIN(NAMED_STRUCT('ldts', load_datetime, 'rs', '{hub._stage.schema}.{hub._stage.table}')).rs AS record_source\n","            FROM {hub._stage.schema}.{hub._stage.table}\n","            WHERE load_datetime IS NOT NULL\n","                AND record_source IS NOT NULL\n","            GROUP BY\n","                {', '.join(hub._stage.columns)}\n","            \"\"\".strip()\n","\n","            # print(ddl_string+\"\\n\")\n","            # vault_ddls.append(ddl_string)\n","            full_name = f\"{hub.schema_name}.{hub.name}\"\n","            vault_ddls[full_name] = ddl_string\n","\n","        for link in tqdm(self._links, desc=\"Generating link DDL string\", unit=\"tables\", total=len(self._links)):\n","            if link._stage is None:\n","                continue\n","\n","            # Collect hub names: anchor hub (if exists) + all join hubs\n","            hub_names = []\n","            if link.anchor.hub:\n","                hub_names.append(link.anchor.hub)\n","            hub_names.extend(join.hub for join in link.hub_mapping)\n","            \n","            virtual_record_source = ';'.join(hub_names)\n","            \n","            link_hk_name = f\"hk_{link.name}\"\n","            hash_sql_string = self._generate_hash_sql({\"name\": link_hk_name, \"columns\": link._stage.columns})\n","\n","            ddl_string = f\"\"\"\n","            CREATE MATERIALIZED LAKE VIEW IF NOT EXISTS `{link.schema_name}`.`{link.name}`\n","            TBLPROPERTIES (delta.enableChangeDataFeed = true)\n","            AS\n","            SELECT\n","                {hash_sql_string},\n","                {', '.join(link._stage.columns)},\n","                MIN(NAMED_STRUCT('ldts', load_datetime, 'rs', '{link._stage.schema}.{link._stage.table}')).ldts AS load_datetime,\n","                MIN(NAMED_STRUCT('ldts', load_datetime, 'rs', '{link._stage.schema}.{link._stage.table}')).rs AS record_source,\n","                '{virtual_record_source}' AS virtual_record_source\n","            FROM {link._stage.schema}.{link._stage.table}\n","            WHERE load_datetime IS NOT NULL\n","                AND record_source IS NOT NULL\n","            GROUP BY\n","                {link_hk_name},\n","                {', '.join(link._stage.columns)}\n","            \"\"\".strip()\n","\n","            # print(ddl_string+\"\\n\")\n","            # vault_ddls.append(ddl_string)\n","\n","            full_name = f\"{link.schema_name}.{link.name}\"\n","            vault_ddls[full_name] = ddl_string\n","\n","        for sat in tqdm(self._sats, desc=\"Generating satellite DDL string\", unit=\"tables\", total=len(self._sats)):\n","            if sat._stage is None or sat._resolved_columns is None:\n","                self._add_issue(\n","                    WarningSeverity.ERROR,\n","                    \"satellite\",\n","                    sat.name,\n","                    \"Satellite not properly processed. Missing stage or resolved columns.\"\n","                )\n","                continue\n","            \n","            hash_diff_name = f\"hd_{sat.name.replace('sat_', '')}\"\n","            parent = self._get_all_parents().get(sat.parent_hub_or_link)\n","\n","            descriptive_cols = sat.resolved_columns\n","\n","            hash_diff_sql = self._generate_hash_sql({\n","                \"name\": hash_diff_name,\n","                \"columns\": descriptive_cols\n","            })\n","\n","            pk_name = f\"pk_{sat.name.replace('sat_', '')}\"\n","            \n","            pk_sql = self._generate_hash_sql({\n","                \"name\": pk_name,\n","                \"columns\": [parent.hash_key_name, \"load_datetime\"]\n","            })\n","\n","            select_parts = [\n","                pk_sql,\n","                parent.hash_key_name,\n","                hash_diff_sql,\n","                *descriptive_cols\n","            ]\n","\n","            '''ddl_string = f\"\"\"\n","            CREATE MATERIALIZED LAKE VIEW IF NOT EXISTS `{sat.schema_name}`.`{sat.name}`\n","            TBLPROPERTIES (delta.enableChangeDataFeed = true)\n","            AS\n","            SELECT\n","\n","                {', '.join(select_parts)},\n","                load_datetime,\n","                '{sat._stage.schema}.{sat._stage.table}' AS record_source,\n","                '{sat.parent_hub_or_link}' AS virtual_record_source\n","            FROM {sat._stage.schema}.{sat._stage.table}\n","            \"\"\".strip()'''\n","\n","            incremental_ver = True\n","\n","            if incremental_ver:\n","                safe_columns = [\n","                    f\"COALESCE(NULLIF(UPPER(TRIM(CAST({col} AS STRING))), ''), '^^NULL^^')\"  # coalesce to some kind of string that is unlikely to appear, formatting based on AutomateDV\n","                    for col in [parent.hash_key_name, \"load_datetime\"]\n","                ]\n","\n","                cols_concat = \", '|', \".join(safe_columns)\n","            \n","                pk = f\"SHA1(CONCAT({cols_concat}))\"\n","\n","                payload_sql = \",\\n                \".join([\n","                    f\"MIN(NAMED_STRUCT('ldts', load_datetime, 'val', {col})).val AS {col}\" \n","                    for col in descriptive_cols\n","                ])\n","\n","                ddl_string = f\"\"\"\n","                CREATE MATERIALIZED LAKE VIEW `{sat.schema_name}`.`{sat.name}`\n","                AS\n","                SELECT\n","                    MIN(NAMED_STRUCT('ldts', load_datetime, 'pk', {pk})).pk AS {pk_name},\n","                    {parent.hash_key_name},\n","                    {hash_diff_sql},\n","                    MIN(NAMED_STRUCT('ldts', load_datetime, 'rs', '{sat._stage.schema}.{sat._stage.table}')).ldts AS load_datetime,\n","                    MIN(NAMED_STRUCT('ldts', load_datetime, 'rs', '{sat._stage.schema}.{sat._stage.table}')).rs AS record_source,\n","                    {payload_sql}\n","                FROM {sat._stage.schema}.{sat._stage.table}\n","                GROUP BY \n","                    {parent.hash_key_name}, \n","                    {hash_diff_name}\n","                \"\"\".strip()\n","                        \n","            else:\n","                ddl_string = f\"\"\"\n","                CREATE MATERIALIZED LAKE VIEW IF NOT EXISTS `{sat.schema_name}`.`{sat.name}`\n","                TBLPROPERTIES (delta.enableChangeDataFeed = true)\n","                AS\n","                SELECT\n","                    {', '.join(select_parts)},\n","                    load_datetime,\n","                    '{sat._stage.schema}.{sat._stage.table}' AS record_source,\n","                    '{sat.parent_hub_or_link}' AS virtual_record_source\n","                \n","                FROM (\n","                    SELECT\n","                        *, \n","                        LAG({hash_diff_name}) OVER (\n","                            PARTITION BY {parent.hash_key_name}\n","                            ORDER BY load_datetime ASC\n","                        ) AS prev_hash_diff,\n","                        ROW_NUMBER() OVER (\n","                            PARTITION BY {parent.hash_key_name}, load_datetime\n","                            ORDER BY record_source ASC\n","                        ) AS rn_same_timestamp\n","                    FROM {sat._stage.schema}.{sat._stage.table}\n","                    WHERE load_datetime IS NOT NULL\n","                    AND record_source IS NOT NULL\n","                )\n","                WHERE ({hash_diff_name} != prev_hash_diff OR prev_hash_diff IS NULL)\n","                AND rn_same_timestamp = 1\n","                \"\"\".strip() \n","\n","            # print(ddl_string+\"\\n\")\n","            # vault_ddls.append(ddl_string)\n","\n","            full_name = f\"{sat.schema_name}.{sat.name}\"\n","            vault_ddls[full_name] = ddl_string\n","\n","        return vault_ddls\n","\n","    def _generate_link_staging_DDL(self) -> List[str]:\n","        ddls: Dict[str, str] = {}\n","        all_hubs = self._get_all_hubs()\n","        \n","        for link in tqdm(self._links, desc=\"Generating link staging DDL string\", unit=\"stage\", total=len(self._links)):\n","            if link._stage is None:\n","                continue\n","            \n","            # Build aliases, hash columns, and joins\n","            hash_columns = []\n","            join_clauses = []\n","            \n","            anchor = link.anchor\n","            anchor_alias = \"t0\"\n","\n","            load_dt_col = self._resolve_load_datetime_column(\n","                anchor.table,          # Still resolve against anchor table\n","                link.load_datetime_column  # âœ… Config lives on Link\n","            )\n","\n","            # Handle anchor's hub contribution (if any)\n","            if anchor.hub:\n","                hub = all_hubs.get(anchor.hub)\n","                if hub:\n","                    hash_sql = self._generate_hash_sql({\n","                        \"name\": hub.hash_key_name,\n","                        \"columns\": [f\"{anchor_alias}.{col}\" for col in anchor.bk_columns]\n","                    })\n","                    hash_columns.append(hash_sql)\n","            \n","            for idx, ref in enumerate(link.hub_mapping):\n","                hub = all_hubs.get(ref.hub)\n","                if hub is None:\n","                    continue\n","                \n","                alias = f\"t{idx + 1}\"  # Start from t1 since t0 is anchor\n","                \n","                # Generate hash key SQL\n","                hash_sql = self._generate_hash_sql({\n","                    \"name\": hub.hash_key_name,\n","                    \"columns\": [f\"{alias}.{col}\" for col in ref.bk_columns]\n","                })\n","                hash_columns.append(hash_sql)\n","                \n","                # Build join clause\n","                join_conditions = \" AND \".join([\n","                    f\"{anchor_alias}.{anchor_col} = {alias}.{join_col}\"\n","                    for anchor_col, join_col in ref.join_on.items()\n","                ])\n","                \n","                join_clauses.append(\n","                    f\"LEFT JOIN {ref.table} {alias} ON {join_conditions}\"\n","                )\n","            \n","            # Build FROM clause\n","            from_clause = f\"{anchor.table} {anchor_alias}\"\n","            \n","            # Build record source\n","            record_sources = [anchor.table.split(\".\")[-1]]\n","            record_sources.extend([j.table.split(\".\")[-1] for j in link.hub_mapping])\n","            record_source = \";\".join(record_sources)\n","            \n","            # Assemble SELECT columns\n","            select_columns = (\n","                hash_columns +\n","                [f\"{anchor_alias}.{load_dt_col} AS load_datetime\"] +\n","                [f\"'{record_source}' AS record_source\"]\n","            )\n","            \n","            columns_sql = \",\\n                \".join(select_columns)\n","            joins_sql = \"\\n            \".join(join_clauses)\n","            \n","            ddl_string = f\"\"\"\n","                CREATE MATERIALIZED LAKE VIEW IF NOT EXISTS `{link._stage.schema}`.`{link._stage.table}`\n","                TBLPROPERTIES (delta.enableChangeDataFeed = true)\n","                AS\n","                SELECT\n","                    {columns_sql}\n","                FROM {from_clause}\n","                {joins_sql}\n","            \"\"\".strip()\n","            \n","            full_name = f\"{link._stage.schema}.{link._stage.table}\"\n","            ddls[full_name] = ddl_string\n","        \n","        return ddls\n","    "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7f2bbcbf-4baf-4a4c-b3fb-4a85d567d8f3"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}